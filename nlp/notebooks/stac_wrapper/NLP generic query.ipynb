{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing tools to analyze free text queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and model downlads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run cell if Stanza is not installed\n",
    "# Stanza - tokenizing, POS tagging, named entities (Apache v2 license)\n",
    "#!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run cell if NLTK is not installed\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run cell if GeoText is not installed\n",
    "# geotext - detect places (MIT license)\n",
    "# geogapy is better, but has dependency issues\n",
    "# !pip install geotext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geotext import GeoText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nouns(text):\n",
    "    \"\"\"\n",
    "    Extract nouns and proper nouns from the query\n",
    "    \"\"\"\n",
    "    nouns = []\n",
    "    doc = nlp(text+\".\")\n",
    "    for sentence in doc.sentences:\n",
    "\n",
    "        # return just noun-related tags\n",
    "        nouns += [word.text for word in sentence.words if word.pos in {\"PROPN\", \"NOUN\"}]\n",
    "    return nouns\n",
    "\n",
    "\n",
    "def get_chunks(text):\n",
    "    \"\"\"\n",
    "    Extract Chunks with NLTK\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    sentences = nltk.sent_tokenize(text) \n",
    "    #print(\"Sentences: \",sentences)\n",
    "    for sent in sentences:\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        # create grammar regex to match the chunks we want\n",
    "        grammar = \"CHUNK: {<NN|NNP><CD>?}\"\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        result = cp.parse(tags)\n",
    "        for subtree in result.subtrees():\n",
    "             if subtree.label() == 'CHUNK': \n",
    "                    # return just noun-related tags\n",
    "                    chunks += [child[0] for child in subtree]\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_where(text):\n",
    "    \"\"\"\n",
    "    Extract geographical named entities\n",
    "    \"\"\"\n",
    "    places = GeoText(text)\n",
    "    # return bbox of place?\n",
    "    if places.cities:\n",
    "        return places.cities[0]\n",
    "    elif places.countries:\n",
    "        return places.countries[0]\n",
    "\n",
    "def get_conditions(text):\n",
    "    \"\"\"\n",
    "    Get numeral, ordinal, percentage and quantity detected by Stanza\n",
    "    \"\"\"\n",
    "    conditions = []\n",
    "    # create chunk matching rules?\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sentences:\n",
    "        ents = sentence.ents\n",
    "        if ents:\n",
    "            conditions += [ent.text for ent in ents \n",
    "                           if ent.type in {\"PERCENT\", \"CARDINAL\", \"ORDINAL\", \"QUANTITY\"}]\n",
    "    return conditions\n",
    "\n",
    "\n",
    "def process_query(text):\n",
    "    \"\"\"\n",
    "    Process the query and return key-value dictionary with extracted parameters\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    # What? - platform/collection\n",
    "    params['what'] = get_nouns(text)\n",
    "    # Where? - GeoNER\n",
    "    params['where'] = get_where(text)\n",
    "    # Conditions? - other variables\n",
    "    params['conditions'] = get_conditions(text)\n",
    "    return params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input  query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#input query text\n",
    "#ex: Sentinel-2 over Ottawa from april to september 2020 with cloud cover less than 20%\n",
    "query=input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_query(query) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
