{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stanza - tokenizing, POS tagging, named entities (Apache v2 license)\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "stanza.download('en')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# geotext - detect places (MIT license)\n",
    "# geogapy is better, but has dependency issues\n",
    "!pip install geotext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saisier une requete en langue naturelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input query text\n",
    "#ex: Sentinel-2 over Ottawa from april to september 2020 with cloud cover less than 20%\n",
    "query=input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract named entities\n",
    "import stanza\n",
    "\n",
    "def get_what(text):\n",
    "    print(\"WHAT:\")\n",
    "    nouns = []\n",
    "    doc = nlp(text+\".\")\n",
    "    for sentence in doc.sentences:\n",
    "        #print(sentence.dependencies)\n",
    "        print(\"word\\t\\tlemma\\t\\tPOS\\t\\tdeprel\")\n",
    "        for word in sentence.words:\n",
    "            print(word.text, \"\\t\\t\", word.lemma, \"\\t\\t\", word.pos, \"\\t\\t\", word.deprel)\n",
    "        # return just noun-related tags\n",
    "        nouns += [word.text for word in sentence.words if word.pos in {\"PROPN\", \"NOUN\"}]\n",
    "    return nouns\n",
    "\n",
    "#try it out\n",
    "get_what(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract named entities\n",
    "import nltk\n",
    "\n",
    "def get_what2(text):\n",
    "    print(\"WHAT:\")\n",
    "    chunks = []\n",
    "    sentences = nltk.sent_tokenize(text) \n",
    "    #print(\"Sentences: \",sentences)\n",
    "    for sent in sentences:\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        print(\"Tokens: \",tokens)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        print(\"POS tags: \",tags)\n",
    "        # create grammar regex to match the chunks we want\n",
    "        grammar = \"CHUNK: {<NN|NNP><CD>?}\"\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        result = cp.parse(tags)\n",
    "        print(\"RegEx grammar matches: \")\n",
    "        for subtree in result.subtrees():\n",
    "             if subtree.label() == 'CHUNK': \n",
    "                    print(subtree)\n",
    "                    # return just noun-related tags\n",
    "                    chunks += [child[0] for child in subtree]\n",
    "    return chunks\n",
    "\n",
    "#try it out\n",
    "get_what2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract geographical named entities\n",
    "from geotext import GeoText\n",
    "\n",
    "def get_where(text):\n",
    "    print(\"WHERE:\")\n",
    "    places = GeoText(text)\n",
    "    print(\"Countries: %s %s\" % (places.countries, places.country_mentions))\n",
    "    print(\"Cities: %s\" % places.cities)\n",
    "    # return bbox of place?\n",
    "    if places.cities:\n",
    "        return places.cities[0]\n",
    "    elif places.countries:\n",
    "        return places.countries[0]\n",
    "\n",
    "#try it out\n",
    "get_where(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_when(text):\n",
    "    print(\"WHEN:\")\n",
    "    return \"\"\n",
    "\n",
    "#try it out\n",
    "get_when(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conditions(text):\n",
    "    print(\"CONDITIONS:\")\n",
    "    conditions = []\n",
    "    # create chunk matching rules?\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sentences:\n",
    "        ents = sentence.ents\n",
    "        if ents:\n",
    "            print([(ent.text, ent.type) for ent in ents])\n",
    "            conditions += [ent.text for ent in ents \n",
    "                           if ent.type in {\"PERCENT\", \"CARDINAL\", \"ORDINAL\", \"QUANTITY\"}]\n",
    "    return conditions\n",
    "\n",
    "#try it out\n",
    "get_conditions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the query and return key-value dictionary with extracted parameters\n",
    "def process_query(text):\n",
    "    # the resulting dictionary\n",
    "    params = {}\n",
    "    # What? - platform/collection\n",
    "    params['what'] = get_what(text)\n",
    "    # Where? - GeoNER\n",
    "    params['where'] = get_where(text)\n",
    "    # When? - detect time\n",
    "    params['when'] = get_when(text)\n",
    "    # Conditions? - other variables\n",
    "    params['conditions'] = get_conditions(text)\n",
    "    return params\n",
    "    \n",
    "process_query(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
