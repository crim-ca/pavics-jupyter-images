{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stanza in /home/bagositi/.local/lib/python3.6/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy in /home/bagositi/.local/lib/python3.6/site-packages (from stanza) (1.19.2)\n",
      "Requirement already satisfied: tqdm in /home/bagositi/.local/lib/python3.6/site-packages (from stanza) (4.50.2)\n",
      "Requirement already satisfied: requests in /home/bagositi/.local/lib/python3.6/site-packages (from stanza) (2.24.0)\n",
      "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.6.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/bagositi/.local/lib/python3.6/site-packages (from stanza) (1.6.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/bagositi/.local/lib/python3.6/site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/bagositi/.local/lib/python3.6/site-packages (from requests->stanza) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/bagositi/.local/lib/python3.6/site-packages (from requests->stanza) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/bagositi/.local/lib/python3.6/site-packages (from requests->stanza) (3.0.4)\n",
      "Requirement already satisfied: setuptools in /home/bagositi/.local/lib/python3.6/site-packages (from protobuf->stanza) (50.3.2)\n",
      "Requirement already satisfied: six>=1.9 in /home/bagositi/.local/lib/python3.6/site-packages (from protobuf->stanza) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Stanza - tokenizing, POS tagging, named entities (Apache v2 license)\n",
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.1.0.json: 122kB [00:00, 13.8MB/s]                    \n",
      "2020-10-20 16:33:33 INFO: Downloading default packages for language: en (English)...\n",
      "2020-10-20 16:33:34 INFO: File exists: /home/bagositi/stanza_resources/en/default.zip.\n",
      "2020-10-20 16:33:39 INFO: Finished downloading models and saved to /home/bagositi/stanza_resources.\n",
      "2020-10-20 16:33:39 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | ewt       |\n",
      "| pos       | ewt       |\n",
      "| lemma     | ewt       |\n",
      "| depparse  | ewt       |\n",
      "| sentiment | sstplus   |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2020-10-20 16:33:39 INFO: Use device: cpu\n",
      "2020-10-20 16:33:39 INFO: Loading: tokenize\n",
      "2020-10-20 16:33:39 INFO: Loading: pos\n",
      "2020-10-20 16:33:40 INFO: Loading: lemma\n",
      "2020-10-20 16:33:40 INFO: Loading: depparse\n",
      "2020-10-20 16:33:41 INFO: Loading: sentiment\n",
      "2020-10-20 16:33:43 INFO: Loading: ner\n",
      "2020-10-20 16:33:44 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')       # This downloads the English models for the neural pipeline\n",
    "nlp = stanza.Pipeline('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/bagositi/.local/lib/python3.6/site-packages (3.5)\n",
      "Requirement already satisfied: regex in /home/bagositi/.local/lib/python3.6/site-packages (from nltk) (2018.1.10)\n",
      "Requirement already satisfied: joblib in /home/bagositi/.local/lib/python3.6/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: click in /home/bagositi/.local/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: tqdm in /home/bagositi/.local/lib/python3.6/site-packages (from nltk) (4.50.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/bagositi/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('popular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: geotext in /home/bagositi/.local/lib/python3.6/site-packages (0.4.0)\n"
     ]
    }
   ],
   "source": [
    "# geotext - detect places (MIT license)\n",
    "# geogapy is better, but has dependency issues\n",
    "!pip install geotext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saisier une requete en langue naturelle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sentinel-2 over Ottawa from april to september 2020 with cloud cover less than 20%\n"
     ]
    }
   ],
   "source": [
    "#input query text\n",
    "#ex: Sentinel-2 over Ottawa from april to september 2020 with cloud cover less than 20%\n",
    "query=input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Sentinel-2 over Ottawa from april to september 2020 with cloud cover less than 20%'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT:\n",
      "word\t\tlemma\t\tPOS\t\tdeprel\n",
      "Sentinel \t\t Sentinel \t\t PROPN \t\t root\n",
      "- \t\t - \t\t PUNCT \t\t punct\n",
      "2 \t\t 2 \t\t NUM \t\t nummod\n",
      "over \t\t over \t\t ADP \t\t case\n",
      "Ottawa \t\t Ottawa \t\t PROPN \t\t nmod\n",
      "from \t\t from \t\t ADP \t\t case\n",
      "april \t\t april \t\t PROPN \t\t nmod\n",
      "to \t\t to \t\t ADP \t\t case\n",
      "september \t\t september \t\t PROPN \t\t nmod\n",
      "2020 \t\t 2020 \t\t NUM \t\t nummod\n",
      "with \t\t with \t\t ADP \t\t case\n",
      "cloud \t\t cloud \t\t NOUN \t\t compound\n",
      "cover \t\t cover \t\t NOUN \t\t nmod\n",
      "less \t\t less \t\t ADJ \t\t advmod\n",
      "than \t\t than \t\t ADP \t\t fixed\n",
      "20 \t\t 20 \t\t NUM \t\t nummod\n",
      "% \t\t % \t\t SYM \t\t nmod\n",
      ". \t\t . \t\t PUNCT \t\t punct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sentinel', 'Ottawa', 'april', 'september', 'cloud', 'cover']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract named entities\n",
    "import stanza\n",
    "\n",
    "def get_what(text):\n",
    "    print(\"WHAT:\")\n",
    "    nouns = []\n",
    "    doc = nlp(text+\".\")\n",
    "    for sentence in doc.sentences:\n",
    "        #print(sentence.dependencies)\n",
    "        print(\"word\\t\\tlemma\\t\\tPOS\\t\\tdeprel\")\n",
    "        for word in sentence.words:\n",
    "            print(word.text, \"\\t\\t\", word.lemma, \"\\t\\t\", word.pos, \"\\t\\t\", word.deprel)\n",
    "        # return just noun-related tags\n",
    "        nouns += [word.text for word in sentence.words if word.pos in {\"PROPN\", \"NOUN\"}]\n",
    "    return nouns\n",
    "\n",
    "#try it out\n",
    "get_what(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT:\n",
      "Tokens:  ['Sentinel-2', 'over', 'Ottawa', 'from', 'april', 'to', 'september', '2020', 'with', 'cloud', 'cover', 'less', 'than', '20', '%']\n",
      "POS tags:  [('Sentinel-2', 'NNP'), ('over', 'IN'), ('Ottawa', 'NNP'), ('from', 'IN'), ('april', 'NN'), ('to', 'TO'), ('september', 'VB'), ('2020', 'CD'), ('with', 'IN'), ('cloud', 'JJ'), ('cover', 'NN'), ('less', 'JJR'), ('than', 'IN'), ('20', 'CD'), ('%', 'NN')]\n",
      "RegEx grammar matches: \n",
      "(CHUNK Sentinel-2/NNP)\n",
      "(CHUNK Ottawa/NNP)\n",
      "(CHUNK april/NN)\n",
      "(CHUNK cover/NN)\n",
      "(CHUNK %/NN)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Sentinel-2', 'Ottawa', 'april', 'cover', '%']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract named entities\n",
    "import nltk\n",
    "\n",
    "def get_what2(text):\n",
    "    print(\"WHAT:\")\n",
    "    chunks = []\n",
    "    sentences = nltk.sent_tokenize(text) \n",
    "    #print(\"Sentences: \",sentences)\n",
    "    for sent in sentences:\n",
    "        tokens = nltk.word_tokenize(sent)\n",
    "        print(\"Tokens: \",tokens)\n",
    "        tags = nltk.pos_tag(tokens)\n",
    "        print(\"POS tags: \",tags)\n",
    "        # create grammar regex to match the chunks we want\n",
    "        grammar = \"CHUNK: {<NN|NNP><CD>?}\"\n",
    "        cp = nltk.RegexpParser(grammar)\n",
    "        result = cp.parse(tags)\n",
    "        print(\"RegEx grammar matches: \")\n",
    "        for subtree in result.subtrees():\n",
    "             if subtree.label() == 'CHUNK': \n",
    "                    print(subtree)\n",
    "                    # return just noun-related tags\n",
    "                    chunks += [child[0] for child in subtree]\n",
    "    return chunks\n",
    "\n",
    "#try it out\n",
    "get_what2(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHERE:\n",
      "Countries: [] OrderedDict([('US', 1)])\n",
      "Cities: ['Ottawa']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ottawa'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract geographical named entities\n",
    "from geotext import GeoText\n",
    "\n",
    "def get_where(text):\n",
    "    print(\"WHERE:\")\n",
    "    places = GeoText(text)\n",
    "    print(\"Countries: %s %s\" % (places.countries, places.country_mentions))\n",
    "    print(\"Cities: %s\" % places.cities)\n",
    "    # return bbox of place?\n",
    "    if places.cities:\n",
    "        return places.cities[0]\n",
    "    elif places.countries:\n",
    "        return places.countries[0]\n",
    "\n",
    "#try it out\n",
    "get_where(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHEN:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_when(text):\n",
    "    print(\"WHEN:\")\n",
    "    return \"\"\n",
    "\n",
    "#try it out\n",
    "get_when(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONDITIONS:\n",
      "[('Ottawa', 'GPE'), ('april to september 2020', 'DATE'), ('less than 20%', 'PERCENT')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['less than 20%']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_conditions(text):\n",
    "    print(\"CONDITIONS:\")\n",
    "    conditions = []\n",
    "    # create chunk matching rules?\n",
    "    doc = nlp(text)\n",
    "    for sentence in doc.sentences:\n",
    "        ents = sentence.ents\n",
    "        if ents:\n",
    "            print([(ent.text, ent.type) for ent in ents])\n",
    "            conditions += [ent.text for ent in ents \n",
    "                           if ent.type in {\"PERCENT\", \"CARDINAL\", \"ORDINAL\", \"QUANTITY\"}]\n",
    "    return conditions\n",
    "\n",
    "#try it out\n",
    "get_conditions(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHAT:\n",
      "word\t\tlemma\t\tPOS\t\tdeprel\n",
      "Sentinel \t\t Sentinel \t\t PROPN \t\t root\n",
      "- \t\t - \t\t PUNCT \t\t punct\n",
      "2 \t\t 2 \t\t NUM \t\t nummod\n",
      "over \t\t over \t\t ADP \t\t case\n",
      "Ottawa \t\t Ottawa \t\t PROPN \t\t nmod\n",
      "from \t\t from \t\t ADP \t\t case\n",
      "april \t\t april \t\t PROPN \t\t nmod\n",
      "to \t\t to \t\t ADP \t\t case\n",
      "september \t\t september \t\t PROPN \t\t nmod\n",
      "2020 \t\t 2020 \t\t NUM \t\t nummod\n",
      "with \t\t with \t\t ADP \t\t case\n",
      "cloud \t\t cloud \t\t NOUN \t\t compound\n",
      "cover \t\t cover \t\t NOUN \t\t nmod\n",
      "less \t\t less \t\t ADJ \t\t advmod\n",
      "than \t\t than \t\t ADP \t\t fixed\n",
      "20 \t\t 20 \t\t NUM \t\t nummod\n",
      "% \t\t % \t\t SYM \t\t nmod\n",
      ". \t\t . \t\t PUNCT \t\t punct\n",
      "WHERE:\n",
      "Countries: [] OrderedDict([('US', 1)])\n",
      "Cities: ['Ottawa']\n",
      "WHEN:\n",
      "CONDITIONS:\n",
      "[('Ottawa', 'GPE'), ('april to september 2020', 'DATE'), ('less than 20%', 'PERCENT')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'what': ['Sentinel', 'Ottawa', 'april', 'september', 'cloud', 'cover'],\n",
       " 'where': 'Ottawa',\n",
       " 'when': '',\n",
       " 'conditions': ['less than 20%']}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process the query and return key-value dictionary with extracted parameters\n",
    "def process_query(text):\n",
    "    # the resulting dictionary\n",
    "    params = {}\n",
    "    # What? - platform/collection\n",
    "    params['what'] = get_what(text)\n",
    "    # Where? - GeoNER\n",
    "    params['where'] = get_where(text)\n",
    "    # When? - detect time\n",
    "    params['when'] = get_when(text)\n",
    "    # Conditions? - other variables\n",
    "    params['conditions'] = get_conditions(text)\n",
    "    return params\n",
    "    \n",
    "process_query(query) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
